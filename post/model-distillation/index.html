<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>模型蒸馏 - Zhou Zaida</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="zhouzaida" /><meta name="description" content="模型蒸馏的出现 大模型的性能比小模型的好，但大模型参数过多，不仅导致内存消耗过大，还导致运行时间过长，而这在实际应用场景中是不可接受的。然而，" /><meta name="keywords" content="Hugo, theme, even" />






<meta name="generator" content="Hugo 0.58.3 with theme even" />


<link rel="canonical" href="https://JustTryItNow.github.io/post/model-distillation/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<link href="/dist/even.ae7674a0.min.css" rel="stylesheet">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.css" integrity="sha256-7TyXnr2YU040zfSP+rEcz29ggW4j56/ujTPwjMzyqFY=" crossorigin="anonymous">


<meta property="og:title" content="模型蒸馏" />
<meta property="og:description" content="模型蒸馏的出现 大模型的性能比小模型的好，但大模型参数过多，不仅导致内存消耗过大，还导致运行时间过长，而这在实际应用场景中是不可接受的。然而，" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://JustTryItNow.github.io/post/model-distillation/" />
<meta property="article:published_time" content="2019-10-12T22:25:40+08:00" />
<meta property="article:modified_time" content="2019-10-12T22:25:40+08:00" />
<meta itemprop="name" content="模型蒸馏">
<meta itemprop="description" content="模型蒸馏的出现 大模型的性能比小模型的好，但大模型参数过多，不仅导致内存消耗过大，还导致运行时间过长，而这在实际应用场景中是不可接受的。然而，">


<meta itemprop="datePublished" content="2019-10-12T22:25:40&#43;08:00" />
<meta itemprop="dateModified" content="2019-10-12T22:25:40&#43;08:00" />
<meta itemprop="wordCount" content="3309">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="模型蒸馏"/>
<meta name="twitter:description" content="模型蒸馏的出现 大模型的性能比小模型的好，但大模型参数过多，不仅导致内存消耗过大，还导致运行时间过长，而这在实际应用场景中是不可接受的。然而，"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Zhou Zaida</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Archives</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">Zhou Zaida</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Archives</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li>
  </ul>
</nav>
    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">模型蒸馏</h1>

      <div class="post-meta">
        <span class="post-time"> 2019-10-12 </span>
        <div class="post-category">
            <a href="/categories/cv/"> cv </a>
            </div>
          <span class="more-meta"> 3309 words </span>
          <span class="more-meta"> 7 mins read </span>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    <div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content always-active">
    <nav id="TableOfContents">
<ul>
<li>
<ul>
<li><a href="#模型蒸馏的出现">模型蒸馏的出现</a></li>
<li><a href="#do-deep-nets-really-need-to-be-deep-2014">Do Deep Nets Really Need to be Deep?(2014)</a></li>
<li><a href="#distilling-the-knowledge-in-a-neural-network-2015">Distilling the Knowledge in a Neural Network(2015)</a></li>
<li><a href="#fitnets-hints-for-thin-deep-nets-2015">FITNETS: HINTS FOR THIN DEEP NETS(2015)</a></li>
<li><a href="#paying-more-attention-to-attention-improving-the-performance-of-convolutional-neural-networks-via-attention-transfer-2017">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer(2017)</a></li>
<li><a href="#like-what-you-like-knowledge-distill-via-neuron-selectivity-transfer-2017">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer(2017)</a></li>
<li><a href="#deep-mutual-learning-2017">Deep Mutual Learning(2017)</a></li>
<li><a href="#training-deep-neural-networks-in-generations-a-more-tolerant-teacher-educates-better-students-2018">Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students(2018)</a></li>
<li><a href="#knowledge-transfer-via-distillation-of-activation-boundaries-formed-by-hidden-neurons-2018">Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons(2018)</a></li>
<li><a href="#on-the-efficacy-of-knowledge-distillation-2019">On the Efficacy of Knowledge Distillation(2019)</a></li>
</ul></li>
</ul>
</nav>
  </div>
</div>
    <div class="post-content">
      

<h2 id="模型蒸馏的出现">模型蒸馏的出现</h2>

<p>大模型的性能比小模型的好，但大模型参数过多，不仅导致内存消耗过大，还导致运行时间过长，而这在实际应用场景中是不可接受的。然而，直接训练小模型，其性能又比大模型低不少，于是模型压缩应运而生。模型压缩包含模型剪枝、参数量化、模型蒸馏等</p>

<h2 id="do-deep-nets-really-need-to-be-deep-2014">Do Deep Nets Really Need to be Deep?(2014)</h2>

<ul>
<li>深层网络真的需要很深吗？<br />
未必。参数一样的深层网络和浅层网络，如果都是从头开始训练，深层网络的性能通常比浅层网络好，但这并不是说浅层网络的性能永远比深层网络差，通过训练好的深层网络带着浅层网络训练，最终性能是差不多的</li>
<li>那训练好的深层网络是怎样带着浅层网络训练的呢？<br />
浅层网络不使用target label作为训练目标，而是使用深层网络生成的logit作为训练目标，即让浅层网络模仿深层网络的行为</li>
<li>那为什么不用prediction(softmax之后)而用logit(softmax之前)呢？

<ul>
<li>logit含有更丰富的信息</li>
<li>不同的logit有可能产生同样的prediction，如果使用prediction，则使得student不很好地学习teacher给出的两者之间的差别</li>
</ul></li>
</ul>

<h2 id="distilling-the-knowledge-in-a-neural-network-2015">Distilling the Knowledge in a Neural Network(2015)</h2>

<ul>
<li>知识（knowledge）是什么，蒸馏（distill）又是什么？<br />
知识是指输入到输出的映射关系。蒸馏是把大模型的知识迁移（transfer）到小模型上</li>
<li>那小模型可以从大模型那里学到什么？<br />
可以学习泛化能力，即学习大模型如何合理地预测一个目标</li>
<li>hard label和soft label那个更好？<br />

<ul>
<li>hard label：训练数据的true label，即one-hot label</li>
<li>soft label：  大模型输出的prediction</li>
<li>soft label相对于hard label具有更丰富的信息，即熵（entropy）更高，从更本质的角度讲，soft
label是大模型直接输出的，有泛化能力的体现，更能反映不同类之间的内在联系。例如是猫狗分类问题，给一张训练图片，里面有一只猫，hard label表示只有猫，而soft label表示这只猫挺像狗的</li>
</ul></li>
<li>那soft label还有其他优点吗？<br />

<ul>
<li>当soft label具有较高的熵时，会使得训练数据之间的梯度的方差更小，所以能使用更少的数据训练小模型和更大的学习率</li>
<li>因为大模型会生成label，所以小模型可以在未标注的数据上训练</li>
</ul></li>
<li>那logit直接经过softmax生成的soft label，信息足够了吗？那该怎么解决？<br />
往往是不够的，因为大模型输出的正确类别仍然会有很大的置信度（即概率），这导致引入soft label的初衷打了折扣。为了使得soft label的概率更平缓，引入了温度系数T，更高的T会使得soft label更平缓</li>
<li>hard label和soft label一起使用会更好</li>
</ul>

<h2 id="fitnets-hints-for-thin-deep-nets-2015">FITNETS: HINTS FOR THIN DEEP NETS(2015)</h2>

<ul>
<li>第三方源码：<a href="https://github.com/adri-romsor/FitNets">https://github.com/adri-romsor/FitNets</a></li>
<li>网络深有什么用呢<br />
深网络在高层能产生更抽象的特征，即特征表示更具不变性</li>
<li>那网络深好训练吗<br />
不好训练，因为更深的网络含有更多连续的非线性层，导致模型急剧非凸，所以不好优化</li>
<li>如何利用深（depth）这个特性使模型更瘦和更深<br />
利用teacher模型中间的特征来指导student的训练</li>
</ul>

<h2 id="paying-more-attention-to-attention-improving-the-performance-of-convolutional-neural-networks-via-attention-transfer-2017">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer(2017)</h2>

<ul>
<li>官方源码：<a href="https://github.com/szagoruyko/attention-transfer">https://github.com/szagoruyko/attention-transfer</a></li>
<li>在蒸馏框架中引入了注意力机制，并提出了activation-based和gradient-based的蒸馏方法</li>
<li>在蒸馏框架中，注意力机制是怎么发挥作用的？<br />
teacher指导student该关注哪些区域</li>
<li>activation-based的提出<br />
关注spatial信息，spatial pixel的激活值越大，那这个pixel越值得注意</li>
<li>gradient-based的提出<br />
从另一个角度思考注意力信息，如果一个像素上的微小改动而造成输出结果的很大变化，这种变化直接反映到梯度的数值上，那么可以说该像素是敏感的，换句话说，这个像素是值得注意的</li>
</ul>

<h2 id="like-what-you-like-knowledge-distill-via-neuron-selectivity-transfer-2017">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer(2017)</h2>

<ul>
<li>提出了对齐student和teacher的中间层的分布</li>
<li>如何衡量两个分布的距离<br />
使用Maximun Mean Discrepancy(MMD)</li>
</ul>

<h2 id="deep-mutual-learning-2017">Deep Mutual Learning(2017)</h2>

<ul>
<li>模型蒸馏是基于怎么样的一种观察<br />
在特定任务中，小网络也许有和大网络同样的表达能力，之所以小网络的性能没大网络好，只是因为小网络比较难优化，于是就有了大网络带着小网络学习的模型蒸馏方法</li>
<li>提出了一帮学生协同学习共同解决问题的方法<br /></li>
<li>为什么一帮未训练的学生能协同学习且最终的性能会比较好<br />
这样的协同策略能让学生学习到更广的最优值，而不是更好的最优值，更广的最优值意味着模型的泛化能力更强</li>
</ul>

<h2 id="training-deep-neural-networks-in-generations-a-more-tolerant-teacher-educates-better-students-2018">Training Deep Neural Networks in Generations: A More Tolerant Teacher Educates Better Students(2018)</h2>

<ul>
<li>In Generations指什么<br />
In Generations的思路是先训练一个teacher(与student的结构一样)，然后再用这个teacher监督student，训练完后，这个student又成了下一阶段的teacher</li>
<li>More Tolerant Teacher有什么用<br />
More Tolerant Teacher虽然准确率没有strict teacher的高，但能提供更多类间相似性信息，能让student更容易去学习teacher所提供的类间相似性信息</li>
<li>提出了secondary information的概念<br />
即图片中的类别相似性信息，注意，这里的类别相似性信息与图片紧密相关，即不同的图片的类别相似性信息是不同的</li>
<li>那如何产生合适的secondary information<br />
目前有两种方法，一种是label smoothing regularization(LSR)，另一种是confidence penalty(CP)，但这两种方法会使得confidence score布满所有的类别而不理会这其中的一些类别是否真有类别相似性，于是作者提出了top score difference(TSD)</li>
</ul>

<h2 id="knowledge-transfer-via-distillation-of-activation-boundaries-formed-by-hidden-neurons-2018">Knowledge Transfer via Distillation of Activation Boundaries Formed by Hidden Neurons(2018)</h2>

<ul>
<li>官方源码：<a href="https://github.com/bhheo/AB_distillation">https://github.com/bhheo/AB_distillation</a></li>
<li>提出了蒸馏teacher激活边界的方法以及activation transfer loss</li>
<li>为什么蒸馏激活边界会有用<br />
有实验证明，神经网络的决策边界是由神经元的激活边界组合而成的</li>
<li>有两种学习策略

<ul>
<li>直接把transfer loss和cross entropy loss组合在一起训练student</li>
<li>先用transfer loss初始化student网络，再用cross entropy loss训练student</li>
<li>为了单独分析transfer loss的影响，作者选用了后者</li>
</ul></li>
<li>在transfer神经元的响应(neural repsonse)时，activation transfer loss为什么会比mean square error更有效<br />
mean square error偏向于student和teacher之间差异比较大的神经元响应，而不能很好地区分weak response和zero response，而这两者之间的差距正是网络分类正确与否的关键。相反地，activation transfer采用了阶跃函数，weak response和strong response都变成了1，即放大了weak response。虽然strong response的transfer被削弱了，但此时activation boundaries却能被准确地transfer。由于activation transfer loss有离散的阶跃函数，故不可导，于是作者使用了其他的等价形式作为loss</li>
<li>如果student和teacher的dimension不同，那要怎么处理<br />
作者是使用全连接层(先把特征reshape成向量，再接上全连接层)。当然，在图像任务上，则可以使用1×1卷积</li>
<li>那为什么student可以通过一个连接层再和teacher进行计算<br />
基于这样的思想：如果student的特征通过简单的转化就可以表示teacher的特征，那说明student和teacher是在同一个level的(即表达能力是接近的)</li>
</ul>

<h2 id="on-the-efficacy-of-knowledge-distillation-2019">On the Efficacy of Knowledge Distillation(2019)</h2>

<ul>
<li>teacher越好，训练出来的student越好吗<br />
不一定，teacher的能力过强，导致student预测的概率分布很难与teacher的相似，即KL散度过大</li>
<li>那该如何解决teacher和student能力不匹配的问题呢

<ul>
<li>teacher和student的capacity不要相差过大</li>
<li>teacher只训练一定的(少于原来的)epoch</li>
<li>teacher训练student的时候可以early stopping</li>
</ul></li>
</ul>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">zhouzaida</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        2019-10-12
        
    </span>
  </p>
  
  
</div>
<footer class="post-footer">
      
      <nav class="post-nav">
        <a class="prev" href="/post/metaclass/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">深入理解Python3元类</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
      </nav>
    </footer>
  </article>
        </div>
        

  

  

      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="justrunordoit@gmail.com" class="iconfont icon-email" title="email"></a>
      <a href="https://github.com/JustTryItNow" class="iconfont icon-github" title="github"></a>
  <a href="https://JustTryItNow.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/olOwOlo/hugo-theme-even">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2017 - 
    2019
    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">zhouzaida</span>
  </span>
</div>
    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3.1.20/dist/jquery.fancybox.min.js" integrity="sha256-XVLffZaxoWfGUEbdzuLi7pwaUJv1cecsQJQqGLe7axY=" crossorigin="anonymous"></script>
<script type="text/javascript" src="/dist/even.26188efa.min.js"></script>








</body>
</html>
